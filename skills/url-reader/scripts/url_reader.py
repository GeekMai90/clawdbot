#!/usr/bin/env python3
"""
URL Reader - ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
ä¸¤å±‚é™çº§ç­–ç•¥ï¼šJina Readerï¼ˆé¦–é€‰ï¼‰ â†’ Playwrightï¼ˆå…œåº•ï¼‰
è‡ªåŠ¨ä¿å­˜åˆ° Obsidian ç½‘ç»œæ”¶è—æ–‡ä»¶å¤¹
"""

import sys
import os
import re
import json
import time
import requests
from urllib.parse import urlparse, urljoin
from datetime import datetime
from pathlib import Path

# â”€â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OBSIDIAN_SAVE_DIR = Path(
    os.path.expanduser(
        "~/Library/Mobile Documents/iCloud~md~obsidian/Documents/GeekMaiOB/00-æ”¶é›†åŒº/ç½‘ç»œæ”¶è—"
    )
)

USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
WECHAT_UA = "Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36 FBAN/FBAV/380.0.0.0.0;FBDV/Google Pixel 7;FBMD/Pixel;FBSN/Android;FBSV/13;"

# å¹³å°å¯¹åº”çš„ Referer
PLATFORM_REFERERS = {
    "xiaohongshu": "https://www.xiaohongshu.com/",
    "wechat":      "https://mp.weixin.qq.com/",
    "zhihu":       "https://www.zhihu.com/",
    "bilibili":    "https://www.bilibili.com/",
}

# é»‘åå•è¯æ±‡ï¼ˆéªŒè¯é¡µé¢ç‰¹å¾ï¼‰
BLACKLIST = ["éªŒè¯ç ", "captcha", "verify", "äººéªŒè¯", "æ»‘åŠ¨éªŒè¯", "è¯·å®ŒæˆéªŒè¯"]


# â”€â”€â”€ 1. å¹³å°è¯†åˆ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def identify_platform(url: str) -> dict:
    """è¯†åˆ« URL æ‰€å±å¹³å°"""
    parsed = urlparse(url)
    netloc = parsed.netloc.lower()

    platforms = {
        "wechat":      ["mp.weixin.qq.com"],
        "xiaohongshu": ["xiaohongshu.com", "xhslink.com"],
        "zhihu":       ["zhihu.com"],
        "douyin":      ["douyin.com"],
        "taobao":      ["taobao.com"],
        "jd":          ["jd.com"],
        "bilibili":    ["bilibili.com"],
    }

    for name, domains in platforms.items():
        if any(d in netloc for d in domains):
            needs_login = name in ("xiaohongshu", "douyin")
            return {"platform": name, "needs_login": needs_login}

    return {"platform": "unknown", "needs_login": False}


# â”€â”€â”€ 2. Firecrawlï¼ˆé¦–é€‰ï¼ŒAIé©±åŠ¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_firecrawl_key() -> str | None:
    """ä»é…ç½®æ–‡ä»¶è¯»å– Firecrawl API Key"""
    key_path = Path(os.path.expanduser("~/.config/firecrawl/api_key"))
    if key_path.exists():
        return key_path.read_text().strip()
    return os.environ.get("FIRECRAWL_API_KEY")


def fetch_firecrawl(url: str) -> str | None:
    """é€šè¿‡ Firecrawl API è·å– Markdown å†…å®¹"""
    api_key = load_firecrawl_key()
    if not api_key:
        print("  âš  Firecrawl API Key æœªé…ç½®ï¼Œè·³è¿‡")
        return None

    try:
        from firecrawl import FirecrawlApp
    except ImportError:
        print("  âš  firecrawl-py æœªå®‰è£…ï¼Œè·³è¿‡")
        return None

    try:
        print("  â†’ è°ƒç”¨ Firecrawlâ€¦")
        app = FirecrawlApp(api_key=api_key)
        result = app.scrape(url)

        # v2 è¿”å› Document å¯¹è±¡ï¼Œç”¨ getattrï¼ˆè¸©å‘ï¼šä¸èƒ½ç”¨ .get()ï¼‰
        markdown = getattr(result, "markdown", None) or (result.get("markdown") if isinstance(result, dict) else None)

        if not markdown or len(markdown) < 100:
            print("  âš  Firecrawl è¿”å›å†…å®¹è¿‡çŸ­ï¼Œé™çº§â€¦")
            return None
        if any(kw in markdown for kw in BLACKLIST):
            print("  âš  Firecrawl è¿”å›éªŒè¯é¡µé¢ï¼Œé™çº§â€¦")
            return None

        # ä» metadata æå– titleï¼Œæ³¨å…¥åˆ°å¼€å¤´ï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œæ–¹ä¾¿ extract_title è§£æï¼‰
        metadata = getattr(result, "metadata", None)
        if metadata:
            title = getattr(metadata, "title", None) or getattr(metadata, "og_title", None)
            if title:
                markdown = f"Title: {title}\n\n{markdown}"

        print(f"  âœ“ Firecrawl è·å–æˆåŠŸï¼ˆ{len(markdown)} å­—ç¬¦ï¼‰")
        return markdown

    except Exception as e:
        print(f"  âš  Firecrawl å¤±è´¥ï¼š{e}ï¼Œé™çº§â€¦")
        return None


# â”€â”€â”€ 3. Jina Readerï¼ˆå¤‡é€‰ï¼Œå…è´¹ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_jina(url: str) -> str | None:
    """é€šè¿‡ Jina Reader è·å– Markdown å†…å®¹"""
    jina_url = f"https://r.jina.ai/{url}"
    # æ³¨æ„ï¼šJina å¯¹å®Œæ•´çš„ Chrome UA ä¼šè¿”å› 403ï¼Œç”¨ç®€çŸ­ UA
    headers = {
        "Accept":          "text/markdown",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Accept-Encoding": "gzip, deflate",
        "User-Agent":      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    }
    try:
        print("  â†’ è°ƒç”¨ Jina Readerâ€¦")
        session = requests.Session()
        session.headers = headers
        resp = session.get(jina_url, timeout=30)
        resp.raise_for_status()
        content = resp.text

        # éªŒè¯å†…å®¹æœ‰æ•ˆæ€§
        if len(content) < 100:
            print("  âš  Jina è¿”å›å†…å®¹è¿‡çŸ­ï¼Œé™çº§â€¦")
            return None
        if any(kw in content for kw in BLACKLIST):
            print("  âš  Jina è¿”å›éªŒè¯é¡µé¢ï¼Œé™çº§â€¦")
            return None

        print(f"  âœ“ Jina è·å–æˆåŠŸï¼ˆ{len(content)} å­—ç¬¦ï¼‰")
        return content

    except Exception as e:
        print(f"  âš  Jina å¤±è´¥ï¼š{e}ï¼Œé™çº§â€¦")
        return None


# â”€â”€â”€ 3. Playwrightï¼ˆå…œåº•ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def fetch_playwright(url: str, platform: str) -> str | None:
    """é€šè¿‡ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–å†…å®¹"""
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("  âš  Playwright æœªå®‰è£…ï¼Œè·³è¿‡")
        print("    å®‰è£…æ–¹æ³•ï¼špip install playwright && playwright install chromium")
        return None

    ua = WECHAT_UA if platform == "wechat" else USER_AGENT

    try:
        print("  â†’ å¯åŠ¨ Playwright æµè§ˆå™¨â€¦")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent=ua)
            page = await context.new_page()

            await page.goto(url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)  # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½

            # æå–æ ‡é¢˜å’Œæ­£æ–‡
            content = await page.evaluate("""() => {
                const title = document.title || '';

                // å°è¯•å¸¸è§çš„ä¸»å†…å®¹é€‰æ‹©å™¨
                const selectors = [
                    '.article-content', '.post-content', '.entry-content',
                    '.rich_text', '.content', 'article', 'main',
                    '#mp-editor', '.pay-unlock-bg',  // å…¬ä¼—å·
                    '.note-content',                  // å°çº¢ä¹¦
                    '.post-text',                     // çŸ¥ä¹
                ];

                let bestEl = null;
                let bestLen = 0;
                for (const sel of selectors) {
                    const el = document.querySelector(sel);
                    if (el && el.innerText.length > bestLen) {
                        bestEl = el;
                        bestLen = el.innerText.length;
                    }
                }

                // éƒ½æ²¡æ‰¾åˆ°å°±ç”¨ body
                const mainEl = bestEl || document.body;

                // æå–å›¾ç‰‡
                const imgs = Array.from(mainEl.querySelectorAll('img'))
                    .map(img => img.src)
                    .filter(src => src && src.startsWith('http'));

                return {
                    title: title,
                    text: mainEl.innerText,
                    images: [...new Set(imgs)]
                };
            }""")

            await browser.close()

            if not content["text"] or len(content["text"]) < 50:
                print("  âš  Playwright è·å–å†…å®¹è¿‡çŸ­")
                return None

            # æ‹¼æ¥æˆç®€å• Markdown
            md = f"# {content['title']}\n\n{content['text']}\n"
            # æŠŠå›¾ç‰‡åŠ å›æ¥
            for img_url in content.get("images", []):
                md += f"\n![image]({img_url})"

            print(f"  âœ“ Playwright è·å–æˆåŠŸï¼ˆ{len(md)} å­—ç¬¦ï¼‰")
            return md

    except Exception as e:
        print(f"  âš  Playwright å¤±è´¥ï¼š{e}")
        return None


# â”€â”€â”€ 4. æå–æ ‡é¢˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_title(markdown: str, url: str) -> str:
    """ä» Markdown ä¸­æå–æ ‡é¢˜"""
    skip_keywords = ["æ¥æº", "source", "url", "http"]

    for line in markdown.split("\n"):
        line = line.strip()
        if not line:
            continue

        # Jina æ ¼å¼ï¼šTitle: xxx
        if line.lower().startswith("title:"):
            title = line.split(":", 1)[1].strip()
            if title and len(title) > 2:
                return title[:80]

        # Markdown æ ‡é¢˜ï¼š# xxx
        if line.startswith("#"):
            title = re.sub(r"^#+\s*", "", line).strip()
            if title and len(title) > 2:
                if any(kw in title.lower() for kw in skip_keywords):
                    continue
                return title[:80]

    # æ²¡æ‰¾åˆ°æ ‡é¢˜å°±ç”¨ hostname
    return urlparse(url).netloc


# â”€â”€â”€ 5. å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_images(markdown: str, images_dir: Path, platform: str) -> tuple[str, int]:
    """ä¸‹è½½ Markdown ä¸­çš„å›¾ç‰‡ï¼Œè¿”å›æ›¿æ¢åçš„ Markdown å’Œä¸‹è½½æ•°é‡"""
    # æå–æ‰€æœ‰å›¾ç‰‡ URL
    # åŒ¹é… Markdown å›¾ç‰‡æ ¼å¼ ![...](url) å’Œç›´æ¥ URL
    img_pattern = re.compile(
        r'!\[([^\]]*)\]\((https?://[^\)]+)\)'
    )
    matches = img_pattern.findall(markdown)

    if not matches:
        # å°è¯•åŒ¹é…ç›´æ¥ URLï¼ˆä»¥å¸¸è§å›¾ç‰‡åç¼€ç»“å°¾ï¼‰
        url_pattern = re.compile(
            r'(https?://[^\s\)]+\.(?:jpg|jpeg|png|gif|webp|bmp)(?:\?[^\s\)]*)?)',
            re.IGNORECASE
        )
        direct_urls = url_pattern.findall(markdown)
        matches = [("image", url) for url in direct_urls]

    if not matches:
        return markdown, 0

    # è®¾ç½® Referer
    headers = {"User-Agent": USER_AGENT}
    if platform in PLATFORM_REFERERS:
        headers["Referer"] = PLATFORM_REFERERS[platform]

    images_dir.mkdir(parents=True, exist_ok=True)
    downloaded = 0

    # å»é‡
    seen_urls = {}
    for alt, img_url in matches:
        if img_url in seen_urls:
            continue

        try:
            print(f"  â†’ ä¸‹è½½å›¾ç‰‡ {downloaded + 1}â€¦")
            resp = requests.get(img_url, headers=headers, timeout=15, stream=True)
            resp.raise_for_status()

            # ç¡®å®šåç¼€
            content_type = resp.headers.get("Content-Type", "image/jpeg")
            ext_map = {
                "image/jpeg": ".jpg",
                "image/png": ".png",
                "image/gif": ".gif",
                "image/webp": ".webp",
            }
            ext = ext_map.get(content_type.split(";")[0].strip(), ".jpg")

            local_name = f"img_{downloaded + 1:02d}{ext}"
            local_path = images_dir / local_name

            with open(local_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)

            seen_urls[img_url] = f"images/{local_name}"
            downloaded += 1
            print(f"    âœ“ ä¿å­˜ä¸º {local_name}")

        except Exception as e:
            print(f"    âš  å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼š{e}")
            seen_urls[img_url] = img_url  # ä¿ç•™åŸ URL

    # æ›¿æ¢ Markdown ä¸­çš„å›¾ç‰‡ URL
    for orig_url, local_path in seen_urls.items():
        markdown = markdown.replace(orig_url, local_path)

    return markdown, downloaded


# â”€â”€â”€ 6. æ¸…ç†ç›®å½•å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sanitize_dirname(name: str) -> str:
    """æ¸…ç†æ ‡é¢˜ç”¨äºç›®å½•å"""
    # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
    for ch in '/\\:*?"<>|':
        name = name.replace(ch, "_")
    # æˆªæ–­é•¿åº¦
    return name[:60].strip()


# â”€â”€â”€ 7. ä¿å­˜å†…å®¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_content(markdown: str, title: str, url: str, platform: str):
    """ä¿å­˜å†…å®¹åˆ° Obsidian"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    time_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    dir_name = f"{date_str}_{sanitize_dirname(title)}"
    save_dir = OBSIDIAN_SAVE_DIR / dir_name

    save_dir.mkdir(parents=True, exist_ok=True)
    images_dir = save_dir / "images"

    # ä¸‹è½½å›¾ç‰‡å¹¶æ›¿æ¢è·¯å¾„
    print("\nğŸ“¸ ä¸‹è½½å›¾ç‰‡â€¦")
    markdown, img_count = download_images(markdown, images_dir, platform)

    # å†™å…¥ content.mdï¼ˆå¸¦ frontmatterï¼‰
    frontmatter = f"""---
title: {title}
source: {url}
platform: {platform}
saved_at: {time_str}
---

"""
    content_path = save_dir / "content.md"
    with open(content_path, "w", encoding="utf-8") as f:
        f.write(frontmatter + markdown)

    return save_dir, content_path, img_count


# â”€â”€â”€ 8. ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼špython url_reader.py <URL>")
        print("ç¤ºä¾‹ï¼špython url_reader.py https://mp.weixin.qq.com/s/xxxxx")
        sys.exit(1)

    url = sys.argv[1].strip()

    print(f"\nğŸŒ URL Reader")
    print(f"   ç›®æ ‡ï¼š{url}\n")

    # 1. å¹³å°è¯†åˆ«
    print("ğŸ” è¯†åˆ«å¹³å°â€¦")
    platform_info = identify_platform(url)
    platform = platform_info["platform"]
    print(f"   å¹³å°ï¼š{platform}")
    if platform_info["needs_login"]:
        print("   âš  æ­¤å¹³å°å¯èƒ½éœ€è¦ç™»å½•æ€ï¼Œå¦‚æœ Jina å¤±è´¥ä¼šç”¨ Playwright å°è¯•")

    # 2. æŠ“å–å†…å®¹ï¼ˆä¸‰å±‚é™çº§ï¼šFirecrawl â†’ Jina â†’ Playwrightï¼‰
    # å¾®ä¿¡å…¬ä¼—å·ç›´æ¥ç”¨ Playwrightï¼ŒFirecrawl/Jina å‡æ— æ³•å¤„ç†
    DIRECT_PLAYWRIGHT_PLATFORMS = {"wechat"}

    if platform in DIRECT_PLAYWRIGHT_PLATFORMS:
        print(f"\nğŸ“¥ {platform} å¹³å°ç›´æ¥ä½¿ç”¨ Playwrightâ€¦")
        content = await fetch_playwright(url, platform)
    else:
        # ç¬¬ä¸€å±‚ï¼šFirecrawl
        print("\nğŸ“¥ æŠ“å–å†…å®¹â€¦")
        content = fetch_firecrawl(url)

        # ç¬¬äºŒå±‚ï¼šJina
        if content is None:
            print("\nğŸ“¥ é™çº§åˆ° Jinaâ€¦")
            content = fetch_jina(url)

        # ç¬¬ä¸‰å±‚ï¼šPlaywright
        if content is None:
            print("\nğŸ“¥ é™çº§åˆ° Playwrightâ€¦")
            content = await fetch_playwright(url, platform)

    if content is None:
        print("\nâŒ æ‰€æœ‰ç­–ç•¥å‡å¤±è´¥ï¼Œæ— æ³•è·å–å†…å®¹")
        sys.exit(1)

    # 3. æå–æ ‡é¢˜
    title = extract_title(content, url)
    print(f"\nğŸ“„ æ ‡é¢˜ï¼š{title}")

    # 4. ä¿å­˜åˆ° Obsidian
    print("\nğŸ’¾ ä¿å­˜åˆ° Obsidianâ€¦")
    save_dir, content_path, img_count = save_content(content, title, url, platform)

    print(f"\nâœ… å®Œæˆï¼")
    print(f"   ğŸ“‚ ç›®å½•ï¼š{save_dir}")
    print(f"   ğŸ“ æ–‡ç¨¿ï¼š{content_path}")
    print(f"   ğŸ–¼  å›¾ç‰‡ï¼š{img_count} å¼ ")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
